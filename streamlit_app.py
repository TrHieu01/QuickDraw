import streamlit as st
from streamlit_drawable_canvas import st_canvas
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode
import numpy as np
import cv2
import torch
import torch.nn.functional as F # C·∫ßn cho Softmax
import os
import mediapipe as mp
from threading import Lock
import time

# --- Import c√°c module c·ªßa b·∫°n ---
# ƒê·∫£m b·∫£o c√°c file n√†y ƒë√£ ƒë∆∞·ª£c t√°ch logic load model ra kh·ªèi global scope
from cnn_model import QuickDrawV2
from classes import QUICKDRAW_CLASSES
from preprocess import preprocess_for_model 

# --- C·∫•u h√¨nh Streamlit ---
st.set_page_config(
    page_title="QuickDraw Classifier (WebRTC)",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 1. Load Model (S·ª≠ d·ª•ng Caching) ---
@st.cache_resource
def load_pytorch_model():
    """T·∫£i model ch·ªâ m·ªôt l·∫ßn v√† tr·∫£ v·ªÅ model ƒë√£ load."""
    model_path = "model/best_model.pth" 
    num_classes = len(QUICKDRAW_CLASSES)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model = QuickDrawV2(num_classes)
    is_loaded = False
    
    try:
        if not os.path.exists(model_path):
            st.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh t·∫°i {model_path}. D·ª± ƒëo√°n s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
            return model, False, DEVICE

        checkpoint = torch.load(model_path, map_location=DEVICE)
        state_dict = checkpoint.get("model_state_dict", checkpoint)
        
        if list(state_dict.keys())[0].startswith('module.'):
            state_dict = {k[7:]: v for k, v in state_dict.items()}
            
        model.load_state_dict(state_dict)
        model.to(DEVICE)
        model.eval()
        is_loaded = True
        st.sidebar.success(f"M√¥ h√¨nh ƒë√£ s·∫µn s√†ng tr√™n {DEVICE}.")
        return model, is_loaded, DEVICE
        
    except Exception as e:
        st.error(f"L·ªñI khi t·∫£i m√¥ h√¨nh: {e}. Vui l√≤ng ki·ªÉm tra file .pth v√† ki·∫øn tr√∫c model.")
        return model, False, DEVICE

MODEL, IS_MODEL_LOADED, DEVICE = load_pytorch_model()

# --- 2. H√†m D·ª± ƒëo√°n Ch√≠nh (T√≠ch h·ª£p logic t·ª´ predict.py v√† th√™m Icon Path) ---
def _predict_drawing(canvas_bgr, model, device, classes):
    """
    Th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω v√† d·ª± ƒëo√°n l·ªõp t·ª´ ·∫£nh v·∫Ω (canvas_bgr), 
    tr·∫£ v·ªÅ k·∫øt qu·∫£ k√®m theo ƒë∆∞·ªùng d·∫´n icon.
    """
    if not IS_MODEL_LOADED:
        return {'prediction': 'Model Not Ready', 'probability': '0.00%', 'top_k': [], 'icon_path': None}

    # 1. Ti·ªÅn x·ª≠ l√Ω ·∫£nh (s·ª≠ d·ª•ng h√†m t·ª´ preprocess.py)
    # G√°n model v√† device v√†o global scope c·ªßa preprocess ƒë·ªÉ h√†m preprocess_for_model c√≥ th·ªÉ d√πng
    # NOTE: ƒê√¢y l√† m·ªôt hack ƒë·ªÉ tr√°nh vi·ªác truy·ªÅn model v√† device v√†o h√†m preprocess_for_model, 
    # nh∆∞ng trong m√¥i tr∆∞·ªùng Streamlit/multithread, c√°ch t·ªët nh·∫•t l√† pass tr·ª±c ti·∫øp.
    # Tuy nhi√™n, do c·∫•u tr√∫c code hi·ªán t·∫°i, ch√∫ng ta t·∫°m th·ªùi d√πng global model/device
    # cho vi·ªác g·ªçi preprocess_for_model.

    # 1. Ti·ªÅn x·ª≠ l√Ω ·∫£nh (s·ª≠ d·ª•ng h√†m t·ª´ preprocess.py)
    input_tensor = preprocess_for_model(canvas_bgr)
    
    if input_tensor is None:
        return {
            'prediction': 'No drawing found',
            'probability': '0.00%',
            'top_k': [],
            'icon_path': None
        }

    # ƒê∆∞a tensor l√™n thi·∫øt b·ªã (CPU/GPU)
    # input_tensor ƒë√£ ƒë∆∞·ª£c to(DEVICE) trong preprocess_for_model, nh∆∞ng th√™m check ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n
    if input_tensor.device != device:
        input_tensor = input_tensor.to(device) 

    # 2. D·ª± ƒëo√°n
    with torch.no_grad():
        output = model(input_tensor)
        
    # 3. T√≠nh to√°n x√°c su·∫•t (Softmax)
    probabilities = F.softmax(output, dim=1)
    
    # 4. L·∫•y d·ª± ƒëo√°n cao nh·∫•t
    max_prob, predicted_index = torch.max(probabilities, 1)
    
    predicted_class = classes[predicted_index.item()]
    confidence = max_prob.item() * 100.0
    
    # 5. L·∫•y Top-K
    top_k_probs, top_k_indices = torch.topk(probabilities, 5)
    
    top_k_results = []
    for i in range(5):
        idx = top_k_indices[0][i].item()
        prob = top_k_probs[0][i].item() * 100.0
        top_k_results.append({
            'class': classes[idx],
            'probability': f"{prob:.2f}%"
        })
        
    # 6. Th√™m Icon Path
    # Chuy·ªÉn t√™n l·ªõp th√†nh t√™n file (ch·ªØ th∆∞·ªùng, thay th·∫ø kho·∫£ng tr·∫Øng b·∫±ng g·∫°ch d∆∞·ªõi)
    icon_filename = f"images/{predicted_class.lower().replace(' ', '_').replace('-', '_')}.png"
    # Ki·ªÉm tra xem file c√≥ t·ªìn t·∫°i kh√¥ng (t√πy ch·ªçn)
    if not os.path.exists(icon_filename):
        icon_filename = None # N·∫øu kh√¥ng t√¨m th·∫•y, ƒë·∫∑t l√† None
        
    return {
        'prediction': predicted_class,
        'probability': f"{confidence:.2f}%",
        'top_k': top_k_results,
        'icon_path': icon_filename
    }


# --- 3. ƒê·ªãnh nghƒ©a B·ªô X·ª≠ l√Ω Video (MediaPipe & Drawing Logic) ---

class AirDrawingProcessor(VideoProcessorBase):
    """
    X·ª≠ l√Ω t·ª´ng khung h√¨nh video t·ª´ camera ƒë·ªÉ nh·∫≠n di·ªán tay v√† v·∫Ω.
    """
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False, max_num_hands=1,
            min_detection_confidence=0.7, min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.CANVAS_W, self.CANVAS_H = 400, 400
        # Canvas l√† n∆°i n√©t v·∫Ω ƒë∆∞·ª£c ghi l·∫°i
        self.canvas = np.full((self.CANVAS_H, self.CANVAS_H, 3), 255, dtype=np.uint8) # Tr·∫Øng
        self.last_point = None
        self.is_drawing = False
        self.has_drawn_since_clear = False
        self.lock = Lock()
        self.prediction_result = None

    # Logic ki·ªÉm tra c·ª≠ ch·ªâ (gi·ªØ nguy√™n)
    def _is_index_finger_extended(self, hand_landmarks):
        index_extended = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].y < hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP].y
        middle_closed = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y > hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP].y
        ring_closed = hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y > hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_PIP].y
        pinky_closed = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y > hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_PIP].y
        return index_extended and middle_closed and ring_closed and pinky_closed

    def _is_open_hand(self, hand_landmarks):
        fingers_and_pips = [
            (self.mp_hands.HandLandmark.INDEX_FINGER_TIP, self.mp_hands.HandLandmark.INDEX_FINGER_PIP),
            (self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP, self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP), 
            (self.mp_hands.HandLandmark.RING_FINGER_TIP, self.mp_hands.HandLandmark.RING_FINGER_PIP), 
            (self.mp_hands.HandLandmark.PINKY_TIP, self.mp_hands.HandLandmark.PINKY_PIP)
        ]
        all_extended = True
        for tip, pip in fingers_and_pips:
            if hand_landmarks.landmark[tip].y > hand_landmarks.landmark[pip].y:
                all_extended = False
                break
        thumb_extended = hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP].x > hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_IP].x
        return all_extended and thumb_extended

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape
        # B·ªé B∆Ø·ªöC cv2.flip(img, 1) - WebRTC ƒë√£ l·∫≠t ·∫£nh, l·∫≠t l·∫°i s·∫Ω g√¢y ng∆∞·ª£c
        
        AIR_AREA_SIZE = 300
        x_start = (w - AIR_AREA_SIZE) // 2
        y_start = (h - AIR_AREA_SIZE) // 2
        x_end = x_start + AIR_AREA_SIZE
        y_end = y_start + AIR_AREA_SIZE
        
        cv2.rectangle(img, (x_start, y_start), (x_end, y_end), (255, 0, 0), 2)
        cv2.putText(img, "AIR DRAWING AREA", (x_start, y_start - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

        results = self.hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(img, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                
                index_finger_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
                x_cam = int(index_finger_tip.x * w)
                y_cam = int(index_finger_tip.y * h)

                pointing = self._is_index_finger_extended(hand_landmarks)
                open_hand = self._is_open_hand(hand_landmarks)
                
                in_draw_area = x_start <= x_cam <= x_end and y_start <= y_cam <= y_end
                                
                if in_draw_area:
                    # √Ånh x·∫° t·ªça ƒë·ªô t·ª´ khung camera sang canvas 400x400
                    x_norm = (x_cam - x_start) / AIR_AREA_SIZE
                    y_norm = (y_cam - y_start) / AIR_AREA_SIZE
                    x_canvas = int(x_norm * self.CANVAS_W)
                    y_canvas = int(y_norm * self.CANVAS_H)
                    
                    if pointing:
                        cv2.circle(img, (x_cam, y_cam), 10, (0, 255, 0), -1) 
                        with self.lock:
                            # V·∫º N√âT M·ªöI: Ch·ªâ v·∫Ω khi ƒëang ·ªü tr·∫°ng th√°i 'is_drawing' (t·ª´ l·∫ßn tr∆∞·ªõc)
                            if self.is_drawing and self.last_point:
                                cv2.line(self.canvas, self.last_point, (x_canvas, y_canvas), (0, 0, 0), 15)
                            
                            self.last_point = (x_canvas, y_canvas)
                            self.is_drawing = True
                            self.has_drawn_since_clear = True
                    else:
                        self.last_point = None
                        self.is_drawing = False
                        
                    # D·ª± ƒëo√°n T·ª± ƒë·ªông (Open Hand)
                    if open_hand and self.has_drawn_since_clear and not self.is_drawing:
                        with self.lock:
                            self.prediction_result = self._predict_and_clear()
                        
                else:
                    self.last_point = None
                    self.is_drawing = False
            else:
                self.last_point = None
                self.is_drawing = False

        # WebRTC Streamer c·∫ßn tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng MediaPipe Frame
        return frame

    def _predict_and_clear(self):
        """Th·ª±c hi·ªán d·ª± ƒëo√°n v√† x√≥a canvas."""
        
        # G·ªåI H√ÄM D·ª∞ ƒêO√ÅN M·ªöI
        result = _predict_drawing(self.canvas, MODEL, DEVICE, QUICKDRAW_CLASSES)
        
        # X√≥a canvas sau khi d·ª± ƒëo√°n
        self.canvas = np.full((self.CANVAS_H, self.CANVAS_H, 3), 255, dtype=np.uint8) # Tr·∫Øng
        self.has_drawn_since_clear = False
        return result

# --- 4. Giao di·ªán Streamlit ---

st.title("QuickDraw Classifier Web App")
st.markdown("Ch·ªçn ch·∫ø ƒë·ªô **'V·∫Ω Chu·ªôt'** (Mouse) ho·∫∑c **'V·∫Ω Kh√¥ng kh√≠'** (Air Drawing) b·∫±ng camera.")

# Tabs
tab1, tab2 = st.tabs(["V·∫Ω Chu·ªôt (Mouse)", "V·∫Ω Kh√¥ng kh√≠ (Air Drawing)"])

# --- Tab 1: V·∫Ω Chu·ªôt (Mouse) ---
with tab1:
    col1_mouse, col2_mouse = st.columns([1, 1])

    with col1_mouse:
        st.header("Canvas V·∫Ω (Chu·ªôt)")
        
        CANVAS_SIZE = 400
        
        with st.sidebar:
            st.subheader("C√¥ng c·ª• V·∫Ω Chu·ªôt")
            stroke_width = st.slider("ƒê·ªô d√†y n√©t v·∫Ω", 10, 40, 20, key="mouse_stroke_width")
        
        # C·∫≠p nh·∫≠t fill_color v√† background_color ƒë·ªÉ ƒë·∫£m b·∫£o n·ªÅn tr·∫Øng tuy·ªát ƒë·ªëi
        canvas_result = st_canvas(
            fill_color="rgba(255, 255, 255, 1)", 
            stroke_width=stroke_width,
            stroke_color="#000000",              
            background_color="#FFFFFF",
            update_streamlit=True,
            height=CANVAS_SIZE,
            width=CANVAS_SIZE,
            drawing_mode="freedraw",
            key="mouse_canvas",
        )

        trigger_prediction_mouse = st.button("D·ª± ƒëo√°n N√©t v·∫Ω (Mouse)", key="btn_mouse_predict")

        if canvas_result.image_data is not None and trigger_prediction_mouse:
            canvas_data_np = canvas_result.image_data.astype(np.uint8)
            rgb_image = canvas_data_np[:, :, :3]
            canvas_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)

            with st.spinner("ƒêang x·ª≠ l√Ω d·ª± ƒëo√°n..."):
                # G·ªåI H√ÄM D·ª∞ ƒêO√ÅN M·ªöI
                result = _predict_drawing(canvas_bgr, MODEL, DEVICE, QUICKDRAW_CLASSES)
            
            st.session_state['mouse_prediction'] = result
            st.session_state['show_mouse_result'] = True
        
    with col2_mouse:
        st.header("K·∫øt qu·∫£ Mouse Drawing")
        
        if 'show_mouse_result' in st.session_state and st.session_state['show_mouse_result']:
            result = st.session_state['mouse_prediction']
            
            if result['prediction'] == 'Model Not Ready':
                st.error("L·ªñI: Kh√¥ng th·ªÉ d·ª± ƒëo√°n v√¨ m√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i.")
            elif result['prediction'] == 'No drawing found':
                st.warning("Kh√¥ng t√¨m th·∫•y n√©t v·∫Ω h·ª£p l·ªá tr√™n canvas. Vui l√≤ng v·∫Ω r√µ h∆°n.")
            else:
                # HI·ªÇN TH·ªä ICON
                if result['icon_path']:
                    st.image(result['icon_path'], caption=f"Icon c·ªßa l·ªõp d·ª± ƒëo√°n: {result['prediction']}", width=100)
                else:
                    st.warning(f"Kh√¥ng t√¨m th·∫•y icon cho l·ªõp '{result['prediction']}' t·∫°i ƒë∆∞·ªùng d·∫´n d·ª± ki·∫øn.")
                
                st.subheader("üèÜ K·∫øt qu·∫£ Ph√¢n lo·∫°i:")
                st.metric(
                    label=f"D·ª± ƒëo√°n Ch√≠nh x√°c nh·∫•t", 
                    value=f"{result['prediction'].upper()}", 
                    delta=f"{result['probability']} Confidence"
                )
                st.markdown("**Top 5 D·ª± ƒëo√°n:**")
                top_k_data = result['top_k']
                for item in top_k_data:
                    st.write(f"- **{item['class']}**: {item['probability']}")

# --- Tab 2: V·∫Ω Kh√¥ng kh√≠ (Air Drawing) ---
with tab2:
    st.header("V·∫Ω Kh√¥ng kh√≠ (Air Drawing) - Camera")
    
    col1_air, col2_air = st.columns([1, 1])

    with col1_air:
        st.info("Khu v·ª±c hi·ªÉn th·ªã camera (WebRTC Streamer)")
        ctx = webrtc_streamer(
            key="air_drawing_stream",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=AirDrawingProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True
        )
        
    with col2_air:
        st.header("Canvas v√† K·∫øt qu·∫£ Air Drawing")
        
        # C·∫≠p nh·∫≠t tr·∫°ng th√°i Canvas hi·ªÉn th·ªã trong session_state
        if ctx.video_processor:
            # D√πng st.session_state ƒë·ªÉ trigger redraw n·∫øu Canvas thay ƒë·ªïi
            if 'air_canvas_version' not in st.session_state:
                st.session_state['air_canvas_version'] = 0

            # C·ªë g·∫Øng l·∫•y canvas v√† k·∫øt qu·∫£ trong khi kh√≥a mutex
            with ctx.video_processor.lock:
                canvas_display = ctx.video_processor.canvas.copy()
                result = ctx.video_processor.prediction_result.copy() if ctx.video_processor.prediction_result else None
            
            st.image(canvas_display, caption="Canvas v·∫Ω b·∫±ng tay", width=400)
            
            if result:
                if result['prediction'] == 'Model Not Ready':
                    st.error("L·ªñI: Kh√¥ng th·ªÉ d·ª± ƒëo√°n v√¨ m√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i.")
                elif result['prediction'] == 'No drawing found':
                    st.warning("V·ª´a r·ªìi kh√¥ng t√¨m th·∫•y n√©t v·∫Ω n√†o. Vui l√≤ng th·ª≠ l·∫°i.")
                else:
                    st.subheader("üèÜ K·∫øt qu·∫£ T·ª± ƒë·ªông:")
                    
                    # HI·ªÇN TH·ªä ICON
                    if result['icon_path']:
                        st.image(result['icon_path'], caption=f"Icon c·ªßa l·ªõp d·ª± ƒëo√°n: {result['prediction']}", width=100)
                    else:
                        st.warning(f"Kh√¥ng t√¨m th·∫•y icon cho l·ªõp '{result['prediction']}' t·∫°i ƒë∆∞·ªùng d·∫´n d·ª± ki·∫øn.")

                    st.metric(
                        label=f"D·ª± ƒëo√°n Ch√≠nh x√°c nh·∫•t", 
                        value=f"{result['prediction'].upper()}", 
                        delta=f"{result['probability']} Confidence"
                    )
                    st.markdown("**Top 5 D·ª± ƒëo√°n:**")
                    for item in result['top_k']:
                        st.write(f"- **{item['class']}**: {item['probability']}")

        else:
            st.info("ƒêang ch·ªù k·∫øt n·ªëi camera...")